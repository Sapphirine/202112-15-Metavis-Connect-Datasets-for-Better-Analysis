{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"metadata_columns_embedding.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovy2oXbAgV_Q","executionInfo":{"status":"ok","timestamp":1638216733592,"user_tz":300,"elapsed":16844,"user":{"displayName":"Guangyu Wu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgdbK7czWOJVMDi8u0hqMDMOPujWewHJPxrTCh=s64","userId":"03054416783263489667"}},"outputId":"aa2105aa-c105-41c9-8072-92a7fe984113"},"source":["from google.colab import drive\n","drive.mount('/content/drive') # , force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"IUeK8fHdgDiQ"},"source":["import pandas as pd \n","import numpy as np\n","import re\n","import os\n","import ast\n","from glob import glob\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_colwidth', 200)\n","\n","def flatten_list(l):\n","  return [item for sublist in l for item in sublist]\n","\n","from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nv98vLFHgoO1"},"source":["metadata_df = pd.read_csv('/content/drive/MyDrive/metavis/socrata_nycopendata_metadata_fetched_20211105.csv')\n","metadata_df['columns'] = metadata_df['columns'].apply(ast.literal_eval)\n","col_metadata_df = metadata_df.explode('columns').reset_index(drop=True)\n","col_metadata_df = col_metadata_df.dropna(subset=['columns']).reset_index(drop=True)\n","col_metadata_df = pd.concat([col_metadata_df['data_uuid'], pd.json_normalize(col_metadata_df['columns'])], axis=1)\n","\n","col_metadata_df = col_metadata_df[ ['data_uuid', 'id', 'tableColumnId', 'name', 'fieldName', 'dataTypeName', 'description'] + [col for col in col_metadata_df.columns if col.startswith('cachedContents.')] ]\n","col_metadata_df['name'] = col_metadata_df['name'].apply(lambda x: x.replace('_',' ')).replace('',np.nan)\n","col_metadata_df['description'] = col_metadata_df['description'].replace('',np.nan)\n","col_metadata_df['description'] = col_metadata_df['name'].fillna('') + col_metadata_df['description'].fillna('').apply(lambda x: ' <SEP> ' + x if len(x)>0 else x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMEhjN7pbMwb"},"source":["tabular_data_uuid_list = metadata_df.query('assetType == \"dataset\" & viewType == \"tabular\"')['data_uuid']\n","tabular_dataset_col_metadata_df = col_metadata_df[col_metadata_df['data_uuid'].isin(tabular_data_uuid_list)]\n","data_uuid_to_name_mapping = metadata_df.set_index('data_uuid')['name'].to_dict()\n","tabular_dataset_col_metadata_df['data_name'] = tabular_dataset_col_metadata_df['data_uuid'].map(data_uuid_to_name_mapping)\n","tabular_dataset_col_metadata_df = tabular_dataset_col_metadata_df[['data_uuid', 'data_name', 'id', 'tableColumnId', 'name', 'fieldName', 'dataTypeName', \n","       'description', 'cachedContents.largest', 'cachedContents.non_null', 'cachedContents.null', 'cachedContents.not_null',\n","       'cachedContents.top', 'cachedContents.smallest', 'cachedContents.count', 'cachedContents.cardinality', 'cachedContents.average',\n","       'cachedContents.sum', 'cachedContents.largest.url', 'cachedContents.smallest.url', 'cachedContents.largest.description', 'cachedContents.smallest.description']]\n","tabular_dataset_col_metadata_df.to_csv('/content/drive/MyDrive/metavis/tabular_dataset_col_metadata_df.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75k1_cbqbe1Z"},"source":["df = pd.read_csv('/content/drive/MyDrive/metavis/tabular_dataset_col_metadata_df.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9mGsHIIYlQX"},"source":["df['name_cleaned'] = df['name'].apply(lambda x: re.sub('[^a-z0-9 ]','',x.lower().strip()).strip())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Or73E18XdtMM","executionInfo":{"status":"ok","timestamp":1638220370844,"user_tz":300,"elapsed":115,"user":{"displayName":"Guangyu Wu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgdbK7czWOJVMDi8u0hqMDMOPujWewHJPxrTCh=s64","userId":"03054416783263489667"}},"outputId":"4c2cf3c3-a5de-45b8-c407-074fa0435135"},"source":["df.name_cleaned.value_counts()[df.name_cleaned.value_counts()>1][:30]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["borough                   642\n","latitude                  413\n","longitude                 411\n","dbn                       392\n","bin                       366\n","year                      356\n","community board           352\n","nta                       346\n","bbl                       346\n","census tract              337\n","council district          326\n","postcode                  300\n","school name               282\n","category                  246\n","grade                     184\n","zip codes                 180\n","community districts       180\n","city council districts    179\n","police precincts          179\n","borough boundaries        178\n","district                  166\n","agency                    165\n","number tested             162\n","mean scale score          160\n","city                      154\n","program type              144\n","level 3                   136\n","level 34                  136\n","level 4                   136\n","level 1                   135\n","Name: name_cleaned, dtype: int64"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","metadata":{"id":"94b9jwI7coH3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"boEPP-8TcoFH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWlqL_RBcoCc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyHRvE3lYlI1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BShknqSV3d0"},"source":["### BERT"]},{"cell_type":"code","metadata":{"id":"jB5RG8joWUF0"},"source":["!pip install transformers --quiet\n","\n","import torch\n","from torch import nn, optim, Tensor\n","from torch.nn import CrossEntropyLoss\n","import torch.nn.functional as F\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","from transformers import BertTokenizer, BertModel\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","def encode(sentences, model, tokenizer, device):\n","    model.eval()\n","    model.to(device)\n","    features = tokenizer(sentences,return_tensors=\"pt\", truncation=True)\n","    with torch.no_grad():\n","        outputs = model(**features)\n","        last_hidden_states = outputs[0]\n","    embedding = torch.mean(last_hidden_states, dim = 1).squeeze()\n","    return  embedding\n","\n","# col_metadata_df['description'] = col_metadata_df['description'].apply(lambda x: x.lower())\n","# col_metadata_df['embedding'] = col_metadata_df['description'].progress_apply(lambda x: encode(x, model, tokenizer, device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5otOnOvV6Qx"},"source":["### GloVe"]},{"cell_type":"code","metadata":{"id":"OKzBd3-KNKXM"},"source":["import gensim\n","import gensim.downloader as api\n","glove_model = api.load('glove-wiki-gigaword-100') # glove-100dimension is 128 MB\n","\n","def remove_domain_frequent_words(input, domain_frequent_words = {}): \n","  output = list(set(input).difference(domain_frequent_words))\n","  return output\n","\n","def get_nan_embedding(wv_model):\n","  na_array = np.empty((wv_model.vector_size,))\n","  na_array[:] = np.nan\n","  return na_array\n","\n","def get_embedding(token, wv_model):\n","  try:\n","    return wv_model.wv[token]\n","  except:\n","    return get_nan_embedding(wv_model)\n","\n","def create_embedding(data, col, domain_frequent_words = {}):\n","  data['_tokens'] = data[col].fillna('').apply(gensim.parsing.preprocessing.remove_stopwords).apply(gensim.utils.simple_preprocess)\n","  data['_tokens_wo_domain_freq'] = data['_tokens'].apply(lambda li: remove_domain_frequent_words(li, domain_frequent_words = domain_frequent_words))\n","  data[col+'__glove_word_embedding'] = data['_tokens_wo_domain_freq'].apply(lambda li: np.nanmean([get_embedding(x, wv_model = glove_model) for x in li], axis=0) if len(li)>0 else get_nan_embedding(wv_model = glove_model) )\n","  data = data.drop(['_tokens','_tokens_wo_domain_freq'],axis=1)\n","  return data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAzQFKapTHyH"},"source":["col_metadata_df = create_embedding(col_metadata_df, 'name', {'nyc', 'data', '<SEP>'})\n","col_metadata_df = create_embedding(col_metadata_df, 'description', {'nyc', 'data', '<SEP>'})\n","col_name_based_word_embedding_for_datasets = col_metadata_df.groupby('data_uuid')['name__glove_word_embedding'].apply(list).apply(lambda li: np.nanmean(li, axis=0)).reset_index().rename(columns = {'name__glove_word_embedding':'colnames__glove_word_embedding'})\n","col_name_based_word_embedding_for_datasets['colnames__glove_word_embedding'] = col_name_based_word_embedding_for_datasets['colnames__glove_word_embedding'].apply(lambda arr: np.nan if all(np.isnan(arr)) else arr)\n","\n","\n","name_and_description_df = metadata_df[['data_uuid','name','description']].copy()\n","name_and_description_df = create_embedding(name_and_description_df, 'name', {'nyc', 'data'})\n","name_and_description_df = create_embedding(name_and_description_df, 'description', {'nyc', 'data'})\n","name_and_description_df = name_and_description_df.rename(columns = {'name__glove_word_embedding':'dataset_name__glove_word_embedding'})\n","name_and_description_df = name_and_description_df.rename(columns = {'description__glove_word_embedding':'dataset_desc__glove_word_embedding'})\n","\n","name_and_description_df['dataset_name__glove_word_embedding'] = name_and_description_df['dataset_name__glove_word_embedding'].apply(lambda arr: np.nan if all(np.isnan(arr)) else arr)\n","name_and_description_df['dataset_desc__glove_word_embedding'] = name_and_description_df['dataset_desc__glove_word_embedding'].apply(lambda arr: np.nan if all(np.isnan(arr)) else arr)\n","\n","\n","\n","dataset_embedding_df = pd.merge(name_and_description_df,  col_name_based_word_embedding_for_datasets, how = 'left')\n","\n","dataset_embedding_df['dataset_embedding'] = dataset_embedding_df[['colnames__glove_word_embedding','dataset_name__glove_word_embedding','dataset_desc__glove_word_embedding']].apply(lambda row: row['colnames__glove_word_embedding'] if not isinstance(row['colnames__glove_word_embedding'],float) else row['dataset_name__glove_word_embedding'] if not isinstance(row['dataset_name__glove_word_embedding'], float) else row['dataset_desc__glove_word_embedding'] ,axis=1)\n","assert(dataset_embedding_df['dataset_embedding'].isnull().mean()==0)\n","\n","dataset_embedding_df[['data_uuid','dataset_embedding']].to_csv(X'/content/drive/MyDrive/metavis/dataset_embedding_v20211117.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tryZncSCYG6h"},"source":["### Read Back and Evaluate Similarity"]},{"cell_type":"code","metadata":{"id":"mXS03QIBVgpd"},"source":["import ast\n","import numpy as np\n","def from_np_array(array_string):\n","  # https://stackoverflow.com/a/42756309\n","  array_string = ','.join(array_string.replace('[ ', '[').split())\n","  return np.array(ast.literal_eval(array_string))\n","test = pd.read_csv('/content/drive/MyDrive/metavis/dataset_embedding_v20211117.csv', converters={'dataset_embedding': from_np_array})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZVDKM8_YD5K"},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","similarities = cosine_similarity(col_metadata_df['embedding'].values)"],"execution_count":null,"outputs":[]}]}